import cv2
import numpy as np
import os

def extract_objects(img):
    """
    ìœ¤ê³½ì„  ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ ê²€ì¶œí•˜ê³ , ê°„ë‹¨í•œ ë¶„ë¥˜(í…ìŠ¤íŠ¸/ë¡œê³  ë“±)ë„ ì‹œë„.
    OCR ë§¤ì¹­ ê¸°ë°˜ìœ¼ë¡œ typeì„ ì •í™•íˆ ì •ë¦¬í•˜ë„ë¡ ê°œì„  ê°€ëŠ¥
    """
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    objects = []
    img_area = img.shape[0] * img.shape[1]

    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        aspect_ratio = w / h if h != 0 else 0
        rel_area = area / img_area

        if area < 1000 or rel_area < 0.0005 or rel_area > 0.7:
            continue  # ë„ˆë¬´ ì‘ê±°ë‚˜ ë„ˆë¬´ í° ê²½ìš° ì œì™¸

        if rel_area < 0.005 and 0.8 < aspect_ratio < 1.2:
            obj_type = "ë¡œê³ "
        elif aspect_ratio > 4.0 or aspect_ratio < 0.25:
            obj_type = "ì„ í˜•ìš”ì†Œ"
        elif 0.3 <= aspect_ratio <= 4.0 and rel_area > 0.003:
            obj_type = "í…ìŠ¤íŠ¸"
        else:
            obj_type = "ê¸°íƒ€"

        objects.append({
            "bbox": (x, y, x + w, y + h),
            "type": obj_type,
            "area": area,
            "aspect_ratio": round(aspect_ratio, 2)
        })

    return objects

def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

def compare_colors(img1, img2):
    hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)
    hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)

    hist1 = cv2.calcHist([hsv1], [0, 1], None, [50, 60], [0, 180, 0, 256])
    hist2 = cv2.calcHist([hsv2], [0, 1], None, [50, 60], [0, 180, 0, 256])

    cv2.normalize(hist1, hist1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
    cv2.normalize(hist2, hist2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)

    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

def get_roi_mean_color(img, bbox):
    x1, y1, x2, y2 = bbox
    roi = img[y1:y2, x1:x2]
    mean_color = cv2.mean(roi)[:3]
    return tuple(map(int, mean_color))

def group_text_lines(texts, y_threshold=30, x_gap_threshold=10, height_variation_ratio=0.8):
    """
    íšŒì „ ì¤‘ì‹¬ê³¼ ë†’ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•´ì„œ ê°™ì€ ì¤„ì˜ í…ìŠ¤íŠ¸ë“¤ì„ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    
    Args:
        texts (list): í…ìŠ¤íŠ¸ ìš”ì†Œë“¤. ê° ìš”ì†ŒëŠ” dictë¡œ, 'bbox', 'center', 'height', 'angle' ë“±ì„ í¬í•¨.
        y_threshold (int): ìˆ˜ì§ ê±°ë¦¬ í—ˆìš©ê°’
        x_gap_threshold (int): ìˆ˜í‰ ê°„ê²© í—ˆìš©ê°’
        height_variation_ratio (float): ë†’ì´ ì°¨ì´ ë¹„ìœ¨ í—ˆìš©ê°’

    Returns:
        list: ì¤„ ë‹¨ìœ„ë¡œ ë¬¶ì¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (list of list of dicts)
    """
    texts = sorted(texts, key=lambda x: x['center'][1])  # y ê¸°ì¤€ ì •ë ¬
    lines = []

    for text in texts:
        cx, cy = text['center']
        th = text['height']
        x1, _, x2, _ = text['bbox']

        placed = False
        for line in lines:
            centers_y = [t['center'][1] for t in line]
            heights = [t['height'] for t in line]
            x2s = [t['bbox'][2] for t in line]

            avg_cy = sum(centers_y) / len(centers_y)
            avg_height = sum(heights) / len(heights)

            y_dist = abs(cy - avg_cy)
            height_diff_ratio = abs(th - avg_height) / max(th, avg_height)
            x_gap = min([max(0.1, x1 - x2p) for x2p in x2s])  # ì—¬ëŸ¬ ìš”ì†Œì™€ ë¹„êµ

            if y_dist < y_threshold and x_gap < x_gap_threshold and height_diff_ratio < height_variation_ratio:
                line.append(text)
                placed = True
                break

        if not placed:
            lines.append([text])

    # ê°™ì€ ì¤„ ë‚´ í…ìŠ¤íŠ¸ë¥¼ x1 ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    for line in lines:
        line.sort(key=lambda t: t['bbox'][0])

    return lines



def match_ocr_to_objects(ocr_elements, object_elements, iou_threshold=0.5):
    matched = []
    for ocr in ocr_elements:
        best_match = None
        best_iou = 0
        for obj in object_elements:
            if obj['type'] != 'í…ìŠ¤íŠ¸':
                continue
            iou = calculate_iou(ocr['bbox'], obj['bbox'])
            if iou > best_iou and iou >= iou_threshold:
                best_iou = iou
                best_match = obj
        if best_match:
            matched.append({
                "text": ocr['text'],
                "ocr_bbox": ocr['bbox'],
                "obj_bbox": best_match['bbox'],
                "iou": best_iou
            })
    return matched

def detect_tables(img, min_table_area=10000, min_w=80, min_h=30, min_aspect=1.2, max_aspect=10.0):
    # 1. Grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # 2. GaussianBlur â†’ Adaptive Threshold
    height, width = gray.shape
    kernel_size = max(int(height * 0.005), int(width * 0.005))
    if kernel_size % 2 == 0:
        kernel_size += 1
    blur = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 1)

    block_size = kernel_size * 2 - 1
    threshold = cv2.adaptiveThreshold(
        blur, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        block_size, 2
    )

    # 3. ë¼í”Œë¼ì‹œì•ˆ í…Œë‘ë¦¬ ê°•ì¡°
    laplacian = cv2.Laplacian(threshold, cv2.CV_64F)
    laplacian = np.uint8(np.absolute(laplacian))

    # 4. ì»¨íˆ¬ì–´ ì „ì²´ ì¶”ì¶œ
    contours, _ = cv2.findContours(laplacian, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # 5. ì‚¬ê°í˜• ëª¨ì–‘ë§Œ ì¶”ì¶œ
    table_boxes = []
    for cnt in contours:
        perimeter = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * perimeter, True)

        # 4ê°œ ì ì„ ê°€ì§„ ë‹¤ê°í˜•ë§Œ (ì‚¬ê°í˜• í›„ë³´)
        if len(approx) == 4:
            x, y, w, h = cv2.boundingRect(approx)
            area = w * h
            aspect = w / (h + 1e-5)
            box = (x, y, x + w, y + h)

            # âœ… ì¢Œí‘œê°’ì— 0ì´ ë‘ ê°œ ì´ìƒ í¬í•¨ë˜ë©´ ì œê±°
            zero_count = sum([1 for v in box if 0 <= v <= 3])
            if zero_count >= 2:
                continue

            if area > min_table_area and w > min_w and h > min_h and min_aspect <= aspect <= max_aspect:
                table_boxes.append((x, y, x + w, y + h))

    return table_boxes

def get_adaptive_params(img_shape):
    h, w = img_shape[:2]

    # ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ scale, min_area ìë™ ì¡°ì •
    scale = max(10, w // 110)                # scaleì€ ë„ˆë¹„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    min_area = (w * h) // 2000               # ì „ì²´ ë©´ì %
    min_area = max(min_area, 3000)           # ë„ˆë¬´ ì‘ì•„ì§€ì§€ ì•Šê²Œ

    kernel_pad = max(1, w // 800)           # ì„ ì˜ ë‘ê»˜ ê¸°ë°˜ dilation kernel ì¡°ì ˆ

    return scale, min_area, kernel_pad

def detect_table_cells_morph(img, is_table_extractor=False):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    gray = clahe.apply(gray)
    gray = cv2.bilateralFilter(gray, 9, 75, 75)

    scale, min_area, pad = get_adaptive_params(img.shape)
    # 1. ì´ì§„í™” (ì„  ê°•ì¡°)
    bin_img = cv2.adaptiveThreshold(~gray, 255,
                                    cv2.ADAPTIVE_THRESH_MEAN_C,
                                    cv2.THRESH_BINARY,
                                    15, -2)

    # 2. ìˆ˜í‰ì„  ì¶”ì¶œ
    h_scale = max(1, bin_img.shape[1] // scale)
    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (h_scale, 1))
    horizontal = cv2.erode(bin_img, h_kernel, iterations=2)
    horizontal = cv2.dilate(horizontal, h_kernel, iterations=pad)  # ğŸ’¡ 1 â†’ 2

    # 3. ìˆ˜ì§ì„  ì¶”ì¶œ
    v_scale = max(1, bin_img.shape[0] // scale)
    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_scale))
    
    # [ìˆ˜ì •] ìš©ë„ì— ë”°ë¥¸ ì¹¨ì‹ ê°•ë„ ì¡°ì ˆ
    # table_extractorìš©ì€ ì–‡ì€ ì„ ë„ íƒì§€í•´ì•¼ í•˜ë¯€ë¡œ 1, main.py(ê¸°ì¡´)ì€ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•´ 2 ìœ ì§€
    v_iterations = 1 if is_table_extractor else 2
    vertical = cv2.erode(bin_img, v_kernel, iterations=v_iterations)
    vertical = cv2.dilate(vertical, v_kernel, iterations=pad)

    # 4. ê·¸ë¦¬ë“œ ê²°í•©
    mask = cv2.add(horizontal, vertical)

    # 5. í‹ˆ ë©”ìš°ê¸°
    close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
    closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, close_kernel, iterations=2)

    # 6. ì»¨íˆ¬ì–´ ì¶”ì¶œ (ê³„ì¸µ êµ¬ì¡°ê¹Œì§€ ì¶”ì )
    contours, hierarchy = cv2.findContours(closed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    cells = []
    for i, c in enumerate(contours):
        x, y, w, h = cv2.boundingRect(c)
        area = w * h

        # [ê°œì„ ] table_extractor ëª¨ë“œì¼ ë•ŒëŠ” ë…ë¦½ì ì¸ ì„ (ì–‡ì€ ë°•ìŠ¤)ë„ í—ˆìš©í•˜ë„ë¡ í•„í„° ì™„í™”
        current_min_area = min_area if not is_table_extractor else 500
        current_min_w = 20 if not is_table_extractor else 3
        
        if area > current_min_area and w > current_min_w and h > 20:
            # ê°€ì¥ ë°”ê¹¥ ì™¸ê³½ì„  ë¬´ì‹œ (ì´ë¯¸ì§€ ì „ì²´ í¬ê¸°ì™€ ìœ ì‚¬í•œ ê²½ìš°)
            if w > img.shape[1] * 0.90 and h > img.shape[0] * 0.90:
                continue
            cells.append((x-10, y-10, x + w+10, y + h+10))

    return sorted(cells, key=lambda b: (b[1], b[0]))  # (y, x) ì •ë ¬

def get_color_regions(img):
    """
    ì´ë¯¸ì§€ë¥¼ ìƒ‰ìƒ ê¸°ì¤€ìœ¼ë¡œ ê±°ì¹ ê²Œ(coarse) êµ¬ì—­í™”í•©ë‹ˆë‹¤.
    ì„œë¡œ ë‹¤ë¥¸ ë°°ê²½ìƒ‰ì„ ê°€ì§„ êµ¬ì—­ ê°„ì˜ ë¬´ë¶„ë³„í•œ ë³‘í•©ì„ ë§‰ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    """
    # ì²˜ë¦¬ ì†ë„ì™€ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ ì¶•ì†Œ ë° ë¸”ëŸ¬
    h, w = img.shape[:2]
    small = cv2.resize(img, (w // 8, h // 8), interpolation=cv2.INTER_AREA)
    blurred = cv2.medianBlur(small, 15)
    
    # Lab ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜ (ì¸ê°„ì˜ ìƒ‰ ì¸ì§€ì™€ ìœ ì‚¬í•œ ê±°ë¦¬ ì¸¡ì •)
    lab = cv2.cvtColor(blurred, cv2.COLOR_BGR2LAB)
    data = lab.reshape((-1, 3)).astype(np.float32)
    
    # K-Means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì£¼ìš” ìƒ‰ìƒ êµ¬ì—­ ë¶„ë¦¬ (K=5 ì •ë„ë©´ ì¶©ë¶„íˆ ê±°ì¹œ êµ¬ì—­í™” ê°€ëŠ¥)
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    _, labels, _ = cv2.kmeans(data, 5, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
    
    # ì›ë˜ í¬ê¸°ë¡œ ë³µì› (Label Map)
    label_img = labels.reshape((lab.shape[0], lab.shape[1])).astype(np.uint8)
    label_img_full = cv2.resize(label_img, (w, h), interpolation=cv2.INTER_NEAREST)
    
    return label_img_full

def get_vertical_mask(img):
    """
    HoughLinesPì™€ Morphologyë¥¼ ê²°í•©í•˜ì—¬ ìˆ˜ì§/ê³¡ì„  êµ¬ë¶„ì„ ì„ ê·¹í•œìœ¼ë¡œ íƒì§€í•©ë‹ˆë‹¤.
    """
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    h_img, w_img = gray.shape[:2]
    
    # 1. ì „ì²˜ë¦¬: ê°•ë ¥í•œ ëŒ€ë¹„ í–¥ìƒ ë° ë…¸ì´ì¦ˆ ì œê±°
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    gray = clahe.apply(gray)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    
    # 2. Canny Edge Detection (ì„  ì„±ë¶„ ì¶”ì¶œ)
    edges = cv2.Canny(blurred, 30, 150, apertureSize=3)
    
    # 3. í™•ë¥ ì  í—ˆí”„ ë³€í™˜ (HoughLinesP)ìœ¼ë¡œ ëª…í™•í•œ ì§ì„  ì°¾ê¸°
    line_mask = np.zeros_like(edges)
    min_line_len = h_img // 50
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, 
                            minLineLength=min_line_len, maxLineGap=10)
    
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            # ìˆ˜ì§ì— ê°€ê¹Œìš´ ì„ ë§Œ ë§ˆìŠ¤í¬ì— ê·¸ë¦¼ (ì‚¬ì„  í¬í•¨)
            angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi)
            if 45 < angle < 135:
                cv2.line(line_mask, (x1, y1), (x2, y2), 255, 3)
                
    # 4. ê¸°ì¡´ Morphology ë°©ì‹ê³¼ ê²°í•© (ë¶ˆì™„ì „í•œ ì„  ë³´ì™„)
    bin_img = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                    cv2.THRESH_BINARY_INV, 21, 5)
    
    v_len = max(25, h_img // 30)
    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_len))
    morph_mask = cv2.morphologyEx(bin_img, cv2.MORPH_CLOSE, v_kernel, iterations=1)
    
    # 5. ìµœì¢… ê²°í•©: í—ˆí”„ ë³€í™˜ ì„  + í˜•íƒœí•™ì  ë§ˆìŠ¤í¬
    final_mask = cv2.bitwise_or(line_mask, morph_mask)
    final_mask = cv2.dilate(final_mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=1)
    
    return final_mask

def has_vertical_separator(v_mask, box1, box2, color_label_map=None):
    """
    ë‘ ë°•ìŠ¤ ì‚¬ì´ì— ë¬¼ë¦¬ì ì¸ ì„  ë˜ëŠ” ìƒ‰ìƒ êµ¬ì—­ì˜ ê²½ê³„ê°€ ìˆëŠ”ì§€ ì²´í¬í•©ë‹ˆë‹¤.
    """
    b1, b2 = (box1, box2) if box1[0] < box2[0] else (box2, box1)
    
    gap_x1, gap_x2 = b1[2], b2[0]
    gap_y1, gap_y2 = min(b1[1], b2[1]), max(b1[3], b2[3])
    char_height = gap_y2 - gap_y1
    
    if char_height <= 0: return False

    # â”€â”€ 1. ìƒ‰ìƒ êµ¬ì—­(Color Zone) ì²´í¬ â”€â”€
    if color_label_map is not None:
        # ë°•ìŠ¤ Aì™€ ë°•ìŠ¤ Bì˜ ì¤‘ì‹¬ ì¢Œí‘œì—ì„œ êµ¬ì—­ ë¼ë²¨ ì¶”ì¶œ
        c1x, c1y = (b1[0] + b1[2]) // 2, (b1[1] + b1[3]) // 2
        c2x, c2y = (b2[0] + b2[2]) // 2, (b2[1] + b2[3]) // 2
        
        # ì¢Œí‘œ ë²”ìœ„ ì•ˆì „ ì²˜ë¦¬
        c1x, c1y = min(max(0, c1x), color_label_map.shape[1]-1), min(max(0, c1y), color_label_map.shape[0]-1)
        c2x, c2y = min(max(0, c2x), color_label_map.shape[1]-1), min(max(0, c2y), color_label_map.shape[0]-1)
        
        label1 = color_label_map[c1y, c1x]
        label2 = color_label_map[c2y, c2x]
        
        # ë‘ ë°•ìŠ¤ê°€ ì„œë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ êµ¬ì—­ì— ìˆë‹¤ë©´ ë³‘í•© ì°¨ë‹¨
        if label1 != label2:
            return True

    # â”€â”€ 2. ë¬¼ë¦¬ì  êµ¬ë¶„ì„  ì²´í¬ â”€â”€
    check_x1 = max(0, gap_x1 - 2)
    check_x2 = min(v_mask.shape[1], gap_x2 + 2)
    roi = v_mask[gap_y1:gap_y2, check_x1:check_x2]
    
    if roi.size > 0:
        v_projection = np.sum(roi > 0, axis=0)
        max_line_component = np.max(v_projection)
        # íƒì§€ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ 50%ë¡œ ë” ì™„í™”
        if max_line_component > char_height * 0.5:
            return True
            
    return False

def filter_overlapping_boxes(table_boxes, iou_thresh=0.3, containment_thresh=2):
    """
    í° ë°•ìŠ¤ ì¤‘ì—ì„œ ë‹¤ë¥¸ ë°•ìŠ¤ì™€ ê²¹ì¹¨(IoU)ì´ ë†’ê³  ì—¬ëŸ¬ ë°•ìŠ¤ë¥¼ í¬í•¨í•˜ë©´ ì œê±°
    """
    def is_contained(inner, outer):
        return (
            inner[0] >= outer[0] and inner[1] >= outer[1] and
            inner[2] <= outer[2] and inner[3] <= outer[3]
        )

    filtered = []
    for i, box in enumerate(table_boxes):
        contained_count = 0
        high_iou_count = 0
        for j, other in enumerate(table_boxes):
            if i == j:
                continue
            if is_contained(other, box):
                contained_count += 1
            if iou(box, other) > iou_thresh:
                high_iou_count += 1

        # í¬í•¨í•˜ëŠ” ë°•ìŠ¤ ìˆ˜ + ê²¹ì¹¨ ë°•ìŠ¤ ìˆ˜ ê¸°ì¤€ í•„í„°ë§
        if contained_count < containment_thresh and high_iou_count < containment_thresh:
            filtered.append(box)

    return filtered


def iou(boxA, boxB):
    # box = (x1, y1, x2, y2)
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interW = max(0, xB - xA)
    interH = max(0, yB - yA)
    interArea = interW * interH

    if interArea == 0:
        return 0.0

    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    iou_value = interArea / float(boxAArea + boxBArea - interArea)
    return iou_value
